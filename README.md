# ğŸ§  Projet Deep Learning : Classification MNIST avec API Flask

Projet de Travaux Pratiques - Deep Learning Engineering  
**DÃ©partement GÃ©nie Informatique, ENSPY**  
**Date**: Septembre 2025

---

## ğŸ“‹ Table des matiÃ¨res

- [Description](#description)
- [Architecture du projet](#architecture-du-projet)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [API Endpoints](#api-endpoints)
- [Docker](#docker)
- [MLflow](#mlflow)
- [RÃ©sultats](#rÃ©sultats)
- [Auteur](#auteur)

---

## ğŸ“– Description

Ce projet implÃ©mente un pipeline complet de Deep Learning, de la conception au dÃ©ploiement, pour la classification des chiffres manuscrits du dataset MNIST.

**Objectifs pÃ©dagogiques** :
- Construction et entraÃ®nement d'un rÃ©seau de neurones avec TensorFlow/Keras
- Suivi des expÃ©rimentations avec MLflow
- CrÃ©ation d'une API REST avec Flask
- Conteneurisation avec Docker
- Versionnement avec Git/GitHub

---

## ğŸ—ï¸ Architecture du projet

```
DEEP/
â”œâ”€â”€ venv/                      # Environnement virtuel Python
â”œâ”€â”€ train_model.py             # Script d'entraÃ®nement du modÃ¨le
â”œâ”€â”€ mnist_model.h5             # ModÃ¨le entraÃ®nÃ© (sauvegardÃ©)
â”œâ”€â”€ app.py                     # API Flask pour les prÃ©dictions
â”œâ”€â”€ test_api.py                # Script de test de l'API
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ Dockerfile                 # Configuration Docker
â”œâ”€â”€ .dockerignore              # Fichiers Ã  ignorer par Docker
â”œâ”€â”€ mlruns/                    # Logs et artifacts MLflow
â””â”€â”€ README.md                  # Documentation (ce fichier)
```

---

## âœ… PrÃ©requis

- Python 3.9 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- Git
- Docker (pour la conteneurisation)
- 4 GB RAM minimum
- 2 GB d'espace disque

---

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone <URL_de_votre_repo>
cd DEEP
```

### 2. CrÃ©er et activer l'environnement virtuel

**Windows (PowerShell)** :
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**Linux/Mac** :
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

---

## ğŸ’» Utilisation

### Ã‰tape 1 : EntraÃ®ner le modÃ¨le

```bash
python train_model.py
```

**Sortie attendue** :
- Un fichier `mnist_model.h5` contenant le modÃ¨le entraÃ®nÃ©
- PrÃ©cision sur les donnÃ©es de test : ~97-98%
- Logs MLflow dans le dossier `mlruns/`

### Ã‰tape 2 : Lancer l'API Flask

```bash
python app.py
```

L'API sera accessible Ã  : `http://localhost:5000`

### Ã‰tape 3 : Tester l'API

Dans un autre terminal :

```bash
python test_api.py
```

---

## ğŸŒ API Endpoints

### 1. Page d'accueil
```http
GET /
```

**RÃ©ponse** :
```json
{
  "message": "API de prÃ©diction MNIST",
  "status": "running",
  "endpoints": { ... }
}
```

### 2. Health Check
```http
GET /health
```

**RÃ©ponse** :
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

### 3. PrÃ©diction
```http
POST /predict
Content-Type: application/json
```

**Corps de la requÃªte** :
```json
{
  "image": [0.0, 0.1, ..., 0.0]  // 784 valeurs entre 0 et 1
}
```

**RÃ©ponse** :
```json
{
  "prediction": 7,
  "confidence": 0.9823,
  "probabilities": [0.001, 0.002, ..., 0.982, ...]
}
```

### Exemple avec curl

```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"image": [0.0, 0.0, ...]}'
```

---

## ğŸ‹ Docker

### Construction de l'image

```bash
docker build -t mnist-api:latest .
```

### Lancement du conteneur

```bash
docker run -d -p 5000:5000 --name mnist-container mnist-api:latest
```

### VÃ©rification

```bash
# Voir les logs
docker logs mnist-container

# Tester l'API
curl http://localhost:5000/health
```

### ArrÃªt et nettoyage

```bash
# ArrÃªter le conteneur
docker stop mnist-container

# Supprimer le conteneur
docker rm mnist-container

# Supprimer l'image
docker rmi mnist-api:latest
```

---

## ğŸ“Š MLflow

### Lancer l'interface MLflow

```bash
mlflow ui
```

AccÃ©dez Ã  : `http://localhost:5000` (ou le port affichÃ©)

### Informations trackÃ©es

- **ParamÃ¨tres** : epochs, batch_size, dropout_rate
- **MÃ©triques** : test_accuracy
- **Artifacts** : modÃ¨le complet

---

## ğŸ“ˆ RÃ©sultats

### Performance du modÃ¨le

| MÃ©trique | Valeur |
|----------|--------|
| PrÃ©cision (test) | ~97-98% |
| Temps d'entraÃ®nement | ~2-3 min (CPU) |
| Taille du modÃ¨le | ~5 MB |
| Nombre de paramÃ¨tres | ~400K |

### Architecture du rÃ©seau

```
Input Layer:  784 neurones (28x28 pixels)
Hidden Layer: 512 neurones (ReLU + Dropout 0.2)
Output Layer: 10 neurones (Softmax)
```

### HyperparamÃ¨tres

- **Optimiseur** : Adam
- **Loss** : Sparse Categorical Crossentropy
- **Epochs** : 5
- **Batch Size** : 128
- **Dropout Rate** : 0.2

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **TensorFlow/Keras** : Construction et entraÃ®nement du modÃ¨le
- **Flask** : API REST
- **MLflow** : Suivi des expÃ©rimentations
- **Docker** : Conteneurisation
- **NumPy** : Manipulation de donnÃ©es
- **Git/GitHub** : Versionnement

---

## ğŸ“ RÃ©ponses aux questions du TP

### Partie 1 - Question 1 : Couches Dense et Dropout

**Dense** : Couche fully-connected oÃ¹ chaque neurone est connectÃ© Ã  tous les neurones de la couche prÃ©cÃ©dente. Apprend les relations complexes entre features.

**Dropout** : Technique de rÃ©gularisation qui dÃ©sactive alÃ©atoirement des neurones pendant l'entraÃ®nement (ici 20%). PrÃ©vient le surapprentissage (overfitting).

**Softmax** : Fonction d'activation utilisÃ©e en sortie pour la classification multi-classes. Convertit les scores en probabilitÃ©s qui somment Ã  1. Permet d'interprÃ©ter la sortie comme une distribution de probabilitÃ© sur les classes.

### Partie 1 - Question 2 : Optimiseur Adam

**Adam** (Adaptive Moment Estimation) amÃ©liore SGD en :
1. **Momentum adaptatif** : Accumule les gradients passÃ©s pour accÃ©lÃ©rer la convergence
2. **Learning rate adaptatif** : Ajuste automatiquement le learning rate pour chaque paramÃ¨tre
3. **Combinaison** : Combine les avantages de RMSprop et Momentum

**Avantages** :
- Convergence plus rapide
- Moins sensible au choix du learning rate initial
- Meilleure performance sur de nombreux problÃ¨mes

### Partie 1 - Question 3 : Vectorisation et calculs par lots

**Vectorisation** : 
- Les images 28Ã—28 sont transformÃ©es en vecteurs de 784 valeurs (`reshape(60000, 784)`)
- Permet d'utiliser des opÃ©rations matricielles optimisÃ©es

**Calculs par lots (batching)** :
- `batch_size=128` : Traite 128 images simultanÃ©ment
- Exploite le parallÃ©lisme des GPU/CPU
- Ã‰quilibre entre vitesse et prÃ©cision du gradient

---

## ğŸš€ AmÃ©liorations futures

- [ ] Ajout de data augmentation
- [ ] Utilisation d'un rÃ©seau convolutionnel (CNN)
- [ ] DÃ©ploiement sur le cloud (AWS, GCP, Azure)
- [ ] Interface web interactive
- [ ] Tests unitaires complets
- [ ] CI/CD avec GitHub Actions

---

## ğŸ“š Ressources

- [Documentation TensorFlow](https://www.tensorflow.org/)
- [Documentation Flask](https://flask.palletsprojects.com/)
- [Documentation MLflow](https://mlflow.org/)
- [Documentation Docker](https://docs.docker.com/)
- [Dataset MNIST](http://yann.lecun.com/exdb/mnist/)

---

## ğŸ‘¨â€ğŸ’» Auteur

**[Votre Nom]**  
Ã‰tudiant en GÃ©nie Informatique  
Ã‰cole Nationale SupÃ©rieure Polytechnique de YaoundÃ© (ENSPY)  

**Contact** : [votre.email@example.com]  
**GitHub** : [github.com/votre-username](https://github.com/votre-username)

---

## ğŸ“„ Licence

Ce projet est rÃ©alisÃ© dans un cadre pÃ©dagogique.

---

## ğŸ™ Remerciements

- Prof. Louis Fippo Fitime, Claude Tinku, Kerolle Sonfack
- DÃ©partement GÃ©nie Informatique, ENSPY
- CommunautÃ© TensorFlow et Keras

---

**Date de crÃ©ation** : Septembre 2025  
**DerniÃ¨re mise Ã  jour** : [Date actuelle]